{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ZN-BAylPoU"
      },
      "source": [
        "# [Serverless Machine Learning in Action](https://www.manning.com/books/serverless-machine-learning-in-action?a_aid=osipov&a_bid=fa913283&)\n",
        "## by Carl Osipov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HPLiT_xlliJ"
      },
      "source": [
        "## **Work In Progress** Source Code for [Chapter 10](https://livebook.manning.com/book/serverless-machine-learning-in-action/chapter-2?a_aid=osipov&a_bid=fa913283&) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWyD1Bt9k26w"
      },
      "source": [
        "## <font color=red>Upload the `BUCKET_ID` file</font>\n",
        "\n",
        "Before proceeding, ensure that you have a backup copy of the `BUCKET_ID` file created in the [Chapter 2](https://colab.research.google.com/github/osipov/smlbook/blob/master/ch2.ipynb) notebook before proceeding. The contents of the `BUCKET_ID` file are reused later in this notebook and in the other notebooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwPOIYDdnXKN"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "assert Path('BUCKET_ID').exists(), \"Place the BUCKET_ID file in the current directory before proceeding\"\n",
        "\n",
        "BUCKET_ID = Path('BUCKET_ID').read_text().strip()\n",
        "os.environ['BUCKET_ID'] = BUCKET_ID\n",
        "os.environ['BUCKET_ID']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ2rTEBfU20C"
      },
      "source": [
        "## **OPTIONAL:** Download and install AWS CLI\n",
        "\n",
        "This is unnecessary if you have already installed AWS CLI in a preceding notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0Vm3p9UkT1"
      },
      "source": [
        "%%bash\n",
        "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "unzip -o awscliv2.zip\n",
        "sudo ./aws/install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xoSKwf7U77e"
      },
      "source": [
        "## Specify AWS credentials\n",
        "\n",
        "Modify the contents of the next cell to specify your AWS credentials as strings. \n",
        "\n",
        "If you see the following exception:\n",
        "\n",
        "`TypeError: str expected, not NoneType`\n",
        "\n",
        "It means that you did not specify the credentials correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaRjFdSoT-q1"
      },
      "source": [
        "import os\n",
        "# *** REPLACE None in the next 2 lines with your AWS key values ***\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = None\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAMFo90AVJuI"
      },
      "source": [
        "## Confirm the credentials\n",
        "\n",
        "Run the next cell to validate your credentials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZqAz5PjS_f1"
      },
      "source": [
        "%%bash\n",
        "aws sts get-caller-identity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66DsruTZWERS"
      },
      "source": [
        "If you have specified the correct credentials as values for the `AWS_ACCESS_KEY_ID` and the `AWS_SECRET_ACCESS_KEY` environment variables, then `aws sts get-caller-identity` used by the previous cell should have returned back the `UserId`, `Account` and the `Arn` for the credentials, resembling the following\n",
        "\n",
        "```\n",
        "{\n",
        "    \"UserId\": \"█████████████████████\",\n",
        "    \"Account\": \"████████████\",\n",
        "    \"Arn\": \"arn:aws:iam::████████████:user/█████████\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wywu4hC-WPxV"
      },
      "source": [
        "## Specify the region\n",
        "\n",
        "Replace the `None` in the next cell with your AWS region name, for example `us-west-2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IowJTSN1e8B-"
      },
      "source": [
        "# *** REPLACE None in the next line with your AWS region ***\n",
        "os.environ['AWS_DEFAULT_REGION'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwJSUTvlfSE0"
      },
      "source": [
        "If you have specified the region correctly, the following cell should return back the region that you have specifies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CssvgRfUSu9"
      },
      "source": [
        "%%bash\n",
        "echo $AWS_DEFAULT_REGION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s0GLfuR0Gtg"
      },
      "source": [
        "## Start with the standard header for PySpark\n",
        "\n",
        "As with previous PySpark jobs, start with the regular imports, argument initialization, and load the DataFrame instance based on the `BUCKET_SRC_PATH` argument specified at job start time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjhzmmJIPu7k"
      },
      "source": [
        "%%writefile dctaxi_feateng.py\n",
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME',\n",
        "                                     'BUCKET_SRC_PATH',\n",
        "                                     'BUCKET_DST_PATH',\n",
        "                                     'SEED',\n",
        "                                     'BINS'])\n",
        "\n",
        "BUCKET_SRC_PATH = args['BUCKET_SRC_PATH']\n",
        "BUCKET_DST_PATH = args['BUCKET_DST_PATH']\n",
        "\n",
        "SEED = int(args['SEED'])\n",
        "BINS = int(args['BINS'])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "logger = glueContext.get_logger()\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "df = ( spark.read.format(\"parquet\")\n",
        "  .option(\"header\", True)\n",
        "  .option(\"inferSchema\", True)\n",
        "  .load(\"{}\".format(BUCKET_SRC_PATH))\n",
        "  )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmEkgD5pHTbB"
      },
      "source": [
        "To retrieve the 4-tuple of minimum as well as maximum latitude and longitude values directly from the dataset you can use the following compound SQL statement via PySpark SQL API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5RAcD6mPyBD"
      },
      "source": [
        "%%writefile -a dctaxi_feateng.py\n",
        "def get_bounding_box(df):\n",
        "  df.createOrReplaceTempView(\"dctaxi\")\n",
        "  row = spark.sql(\"\"\"\n",
        "  SELECT \n",
        "      MIN(lat) AS min_lat, \n",
        "      MAX(lat) AS max_lat, \n",
        "      MIN(lon) AS min_lon, \n",
        "      MAX(lon) AS max_lon \n",
        "\n",
        "  FROM (\n",
        "\n",
        "    SELECT \n",
        "      MIN(origin_block_latitude_double)      AS lat,\n",
        "      MIN(origin_block_longitude_double)     AS lon FROM dctaxi UNION\n",
        "\n",
        "    SELECT \n",
        "      MIN(destination_block_latitude_double) AS lat, \n",
        "      MIN(destination_block_longitude_double)AS lon FROM dctaxi UNION\n",
        "\n",
        "    SELECT \n",
        "      MAX(origin_block_latitude_double)      AS lat, \n",
        "      MAX(origin_block_longitude_double)     AS lon FROM dctaxi UNION\n",
        "\n",
        "    SELECT \n",
        "      MAX(destination_block_latitude_double) AS lat, \n",
        "      MAX(destination_block_longitude_double)AS lon FROM dctaxi\n",
        "\n",
        "  ) LIMIT 1\n",
        "  \"\"\".replace('\\n', '')).first()\n",
        "  return row.min_lat, row.max_lat, row.min_lon, row.max_lon\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9_04LJFHtGk"
      },
      "source": [
        "The SQL statement executed against the cleaned up dataset from the chapter 4,  should output the following values:\n",
        "\n",
        "| min_lat      | max_lat | min_lon | max_lon |\n",
        "| ------------ | ------- | ------- | ------- |\n",
        "| 38.81138      | 38.994909       | -77.113633      | -76.910012       |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVdcuXukfxmc"
      },
      "source": [
        "Assuming that `linspace` was used to define binning intervals for the latitude and longitude coordinates in `lat_bins`, and `lon_bins` variables respectively, the built-in PySpark `pyspark.ml.feature.Bucketizer` class can transform the source PySpark DataFrame (`df`) and create columns that contain a representation of the binned coordinates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG-w0VkAP7vB"
      },
      "source": [
        "%%writefile -a dctaxi_feateng.py\n",
        "def add_binned_coordinate_features(df, lat_bins, lon_bins):\n",
        "  from pyspark.ml.feature import Bucketizer\n",
        "  olat_bucketizer = Bucketizer(splits = lat_bins, \n",
        "                              inputCol=\"origin_block_latitude_double\", \n",
        "                              outputCol=\"origin_latitude_bin\")\n",
        "  olat_bucketizer.setHandleInvalid(\"keep\")\n",
        "\n",
        "  olon_bucketizer = Bucketizer(splits = lon_bins, \n",
        "                              inputCol=\"origin_block_longitude_double\", \n",
        "                              outputCol=\"origin_longitude_bin\")\n",
        "  olon_bucketizer.setHandleInvalid(\"keep\")\n",
        "\n",
        "  dlat_bucketizer = Bucketizer(splits = lat_bins, \n",
        "                              inputCol=\"destination_block_latitude_double\", \n",
        "                              outputCol=\"destination_latitude_bin\")\n",
        "  dlat_bucketizer.setHandleInvalid(\"keep\")\n",
        "\n",
        "  dlon_bucketizer = Bucketizer(splits = lon_bins, \n",
        "                              inputCol=\"destination_block_longitude_double\", \n",
        "                              outputCol=\"destination_longitude_bin\")\n",
        "  dlon_bucketizer.setHandleInvalid(\"keep\")\n",
        "\n",
        "  df = olat_bucketizer.transform(df)\n",
        "  df = olon_bucketizer.transform(df)\n",
        "  df = dlat_bucketizer.transform(df)\n",
        "  df = dlon_bucketizer.transform(df)\n",
        "\n",
        "  return df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czaM-qeygLC4"
      },
      "source": [
        "While PySpark included `OneHotEncoder` class can be used to convert the categorical values into a one hot encoded representation, the class has an important disadvantage. The output of the `OneHotEncoder` is a Spark-specific `SparseVector` data structure that does not natively convert into the CSV format and has limited utility outside of Spark. Fortunately, it is straightforward to implement one hot encoding in PySpark (`map_label_enc_to_one_hot_enc` below), to mirror the encoding behavior from frameworks like Pandas or Numpy, creating a column per distinct value of a one hot encoded categorical feature. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpKYuF7Lygdz"
      },
      "source": [
        "%%writefile -a dctaxi_feateng.py\n",
        "from pyspark.sql import Row\n",
        "def map_label_enc_to_one_hot_enc(cols, upper, lower = 0):\n",
        "    def label_enc_to_one_hot_enc(row):\n",
        "        row_dict = row.asDict()\n",
        "        for col in cols:\n",
        "            #create a dictionary with every column as key\n",
        "            col_dict = dict(zip( [col + \"_\" + str(i) for i in range(lower, upper) ],\n",
        "                                [0] * (upper - lower) ) )\n",
        "                        \n",
        "            #set the one hot column to be 1\n",
        "            col_dict.update({col + \"_\" + str(int(row_dict[col])): 1}) if row_dict[col] else None\n",
        "    \n",
        "            row_dict.update(col_dict)\n",
        "        \n",
        "        return Row(**row_dict)\n",
        "    return label_enc_to_one_hot_enc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHFX4QQEgk8Y"
      },
      "source": [
        "Once the `map_label_enc_to_one_hot_enc` function is defined, it can be used with the Spark RDD (Resilient Distribute Dataset) `map` method to one hot encode one or more columns. Although the method allows for multiple integer columns to be one hot encoded, the following example applies the function to just the `dow_integer` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RBzQrNlQM-D"
      },
      "source": [
        "%%writefile -a dctaxi_feateng.py\n",
        "def add_dow_feature(df):\n",
        "  from pyspark.sql.functions import dayofweek\n",
        "  from pyspark.sql.types import IntegerType\n",
        "  df = ( df\n",
        "          .withColumn('dow_integer', \n",
        "                      dayofweek('origindatetime_ts').cast(IntegerType()) )\n",
        "          .fillna( 0, ['dow_integer'] ) )\n",
        "\n",
        "  rdd = df.rdd.map(map_label_enc_to_one_hot_enc(['dow_integer'], \n",
        "                                                lower = 1, \n",
        "                                                upper = 8)) # 7 days of the week\n",
        "  return rdd.toDF()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9PGySSSQKom"
      },
      "source": [
        "%%writefile -a dctaxi_feateng.py\n",
        "min_lat, max_lat, min_lon, max_lon = get_bounding_box(df)  \n",
        "\n",
        "import numpy as np\n",
        "lat_bins = np.linspace(min_lat, max_lat, BINS + 1)\n",
        "lon_bins = np.linspace(min_lon, max_lon, BINS + 1)\n",
        "\n",
        "df = add_binned_coordinate_features(df, lat_bins, lon_bins)\n",
        "\n",
        "df = add_dow_feature(df)\n",
        "\n",
        "cols = ['fareamount_double',\n",
        "        'origin_latitude_bin', \n",
        "        'origin_longitude_bin', \n",
        "        'destination_latitude_bin', \n",
        "        'destination_longitude_bin',\n",
        "        'dow_integer'] \\\n",
        "+ list(filter(lambda s: s.startswith('dow_integer_') \\\n",
        "              if s is not None else False, df.schema.names))\n",
        "\n",
        "df = df.select(cols)\n",
        "\n",
        "(df\n",
        " .write\n",
        " .option('header', True)\n",
        " .csv(\"{}\".format(BUCKET_DST_PATH), mode=\"overwrite\"))\n",
        "\n",
        "from pyspark.sql.functions import spark_partition_id\n",
        "summary_df = (df.withColumn(\"_partitionId\", spark_partition_id())\n",
        "    .groupBy(\"_partitionId\")\n",
        "    .count())\n",
        "\n",
        "(summary_df\n",
        "  .repartition(1)\n",
        "  .write\n",
        "  .option('header', True)\n",
        "  .csv(\"{}/summary\".format(BUCKET_DST_PATH), mode=\"overwrite\"))\n",
        "\n",
        "job.commit()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3sP9ctahDxI"
      },
      "source": [
        "## Download a Utility Script to Run PySpark Jobs\n",
        "\n",
        "The script is downloaded as `utils.sh` and is loaded in the upcoming cells using `source utils.sh` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG8v9KnohEFK"
      },
      "source": [
        "%%bash\n",
        "wget -q https://raw.githubusercontent.com/osipov/smlbook/master/utils.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RPqpwIRh3qf"
      },
      "source": [
        "## Run the PySpark job as specified by `dctaxi_feateng.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvOP7keP5Coc"
      },
      "source": [
        "%%bash\n",
        "source utils.sh\n",
        "\n",
        "PYSPARK_SRC_NAME=dctaxi_feateng.py \\\n",
        "PYSPARK_JOB_NAME=dc-taxi-feateng-job \\\n",
        "BUCKET_SRC_PATH=s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/parquet/clean \\\n",
        "BUCKET_DST_PATH=s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/csv/feateng \\\n",
        "BINS=8 \\\n",
        "SEED=42 \\\n",
        "run_job"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEphNkp1ncGp"
      },
      "source": [
        "In case of a successful completion, the last cell should have produced an output similar to the following:\n",
        "\n",
        "```\n",
        "2021-06-01 23:34:56       1840 dctaxi_feateng.py\n",
        "{\n",
        "    \"JobName\": \"dc-taxi-feateng-job\"\n",
        "}\n",
        "{\n",
        "    \"Name\": \"dc-taxi-feateng-job\"\n",
        "}\n",
        "{\n",
        "    \"JobRunId\": \"jr_59eee7f229f448b39286f1bd19428c9082aaf6bed232342cc05e68f9246d131e\"\n",
        "}\n",
        "Waiting for the job to finish...............SUCCEEDED\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYt-3VQUk5jv"
      },
      "source": [
        "Copyright 2021 CounterFactual.AI LLC. All Rights Reserved.\n",
        "\n",
        "Licensed under the GNU General Public License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://github.com/osipov/smlbook/blob/master/LICENSE\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ]
}