{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ZN-BAylPoU"
      },
      "source": [
        "# [Serverless Machine Learning in Action](https://www.manning.com/books/serverless-machine-learning-in-action?a_aid=osipov&a_bid=fa913283&)\n",
        "## by Carl Osipov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HPLiT_xlliJ"
      },
      "source": [
        "## **Work In Progress** Source Code for [Chapter 4](https://livebook.manning.com/book/serverless-machine-learning-in-action/chapter-2?a_aid=osipov&a_bid=fa913283&) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWyD1Bt9k26w"
      },
      "source": [
        "## <font color=red>Upload the `BUCKET_ID` file</font>\n",
        "\n",
        "Before proceeding, ensure that you have a backup copy of the `BUCKET_ID` file created in the [Chapter 2](https://colab.research.google.com/github/osipov/smlbook/blob/master/ch2.ipynb) notebook before proceeding. The contents of the `BUCKET_ID` file are reused later in this notebook and in the other notebooks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwPOIYDdnXKN"
      },
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "assert Path('BUCKET_ID').exists(), \"Place the BUCKET_ID file in the current directory before proceeding\"\n",
        "\n",
        "BUCKET_ID = Path('BUCKET_ID').read_text().strip()\n",
        "os.environ['BUCKET_ID'] = BUCKET_ID\n",
        "os.environ['BUCKET_ID']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ2rTEBfU20C"
      },
      "source": [
        "## **OPTIONAL:** Download and install AWS CLI\n",
        "\n",
        "This is unnecessary if you have already installed AWS CLI in a preceding notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0Vm3p9UkT1"
      },
      "source": [
        "%%bash\n",
        "curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
        "unzip -o awscliv2.zip\n",
        "sudo ./aws/install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xoSKwf7U77e"
      },
      "source": [
        "## Specify AWS credentials\n",
        "\n",
        "Modify the contents of the next cell to specify your AWS credentials as strings. \n",
        "\n",
        "If you see the following exception:\n",
        "\n",
        "`TypeError: str expected, not NoneType`\n",
        "\n",
        "It means that you did not specify the credentials correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaRjFdSoT-q1"
      },
      "source": [
        "import os\n",
        "# *** REPLACE None in the next 2 lines with your AWS key values ***\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = None\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAMFo90AVJuI"
      },
      "source": [
        "## Confirm the credentials\n",
        "\n",
        "Run the next cell to validate your credentials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZqAz5PjS_f1"
      },
      "source": [
        "%%bash\n",
        "aws sts get-caller-identity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66DsruTZWERS"
      },
      "source": [
        "If you have specified the correct credentials as values for the `AWS_ACCESS_KEY_ID` and the `AWS_SECRET_ACCESS_KEY` environment variables, then `aws sts get-caller-identity` used by the previous cell should have returned back the `UserId`, `Account` and the `Arn` for the credentials, resembling the following\n",
        "\n",
        "```\n",
        "{\n",
        "    \"UserId\": \"█████████████████████\",\n",
        "    \"Account\": \"████████████\",\n",
        "    \"Arn\": \"arn:aws:iam::████████████:user/█████████\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wywu4hC-WPxV"
      },
      "source": [
        "## Specify the region\n",
        "\n",
        "Replace the `None` in the next cell with your AWS region name, for example `us-west-2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IowJTSN1e8B-"
      },
      "source": [
        "# *** REPLACE None in the next line with your AWS region ***\n",
        "os.environ['AWS_DEFAULT_REGION'] = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwJSUTvlfSE0"
      },
      "source": [
        "If you have specified the region correctly, the following cell should return back the region that you have specifies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CssvgRfUSu9"
      },
      "source": [
        "%%bash\n",
        "echo $AWS_DEFAULT_REGION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOVYU1csFvOC"
      },
      "source": [
        "## Review the summary statistics of the cleaned up dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNBE81hcDel0"
      },
      "source": [
        "At the conclusion of Chapter 3, along with the cleaned up version of the dataset, the PySpark job saved some metadata information with the statistical description for the dataset, including the total number of rows, means, standard deviations, minimums, and maximums for every column of values in the dataset. To read this information into your Jupyter notebook using Pandas, execute the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "039nCToiDiDS"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(f\"s3://dc-taxi-{os.environ['BUCKET_ID']}-{os.environ['AWS_DEFAULT_REGION']}/parquet/clean_summary/*\")\n",
        "\n",
        "print(df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9-NdzcUDomk"
      },
      "source": [
        "To get started using the data frame, you can index the data frame using the summary column and take a look at the result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PgkBBa3Dphf"
      },
      "source": [
        "summary_df = df.set_index('summary')\n",
        "summary_df = summary_df._get_numeric_data()\n",
        "summary_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEF5D3PcDtGq"
      },
      "source": [
        "Let’s save the size of the dataset (i.e. in terms of the number of the rows) to a separate variable which can be used later:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oregbzC6DvZ0"
      },
      "source": [
        "ds_size = summary_df.loc['count'].astype(int).max()\n",
        "print(ds_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pt3MwO-Dv37"
      },
      "source": [
        "Since the upcoming part of the chapter is focused on sampling from the data, go ahead and create two separate series to gather the population mean (`mu`):\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lodX6VqDzui"
      },
      "source": [
        "mu = summary_df.loc['mean']\n",
        "print(mu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRqqgtEVD1jx"
      },
      "source": [
        "and the standard deviation (`sigma`) statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5mmIwwGD3nG"
      },
      "source": [
        "sigma = summary_df.loc['stddev']\n",
        "print(sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB5a9ueDD5xF"
      },
      "source": [
        "## Choosing the right sample size for the test dataset\n",
        "\n",
        "You can obtain the number of records for the 30% / 15% / 10% / 1% / 0.1% test partitions (specified by the `fractions` variable)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OBlA7tsD8Mk"
      },
      "source": [
        "fractions = [.3, .15, .1, .01, .001]\n",
        "print([ds_size * fraction for fraction in fractions])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTp0QdrND-Yo"
      },
      "source": [
        "To find power of two estimates for the fractions of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOMk3BnJEBge"
      },
      "source": [
        "from math import log, ceil, floor, sqrt\n",
        "ranges = [floor(log(ds_size * fraction, 2)) for fraction in fractions]\n",
        "print(ranges)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKQ0hCrWEDo7"
      },
      "source": [
        "Let's continue using the upper part of the range with the largest sample size of $2^{24} = 16,777,216$ and a smaller part of the range of $2^{14} = 16,384$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEL-W1sYEHVM"
      },
      "source": [
        "sample_size_upper, sample_size_lower = max(ranges) + 1, min(ranges) - 1\n",
        "print(sample_size_upper, sample_size_lower)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C80nNRbEIya"
      },
      "source": [
        "Given a range, you can figure out how well the range approximates fractions of the dataset:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRYPClD1ENMk"
      },
      "source": [
        "sizes = [2 ** i for i in range(sample_size_lower, sample_size_upper)]\n",
        "original_sizes = sizes\n",
        "fracs = [ size / ds_size for size in sizes]\n",
        "print(*[(idx, sample_size_lower + idx, frac, size) for idx, (frac, size) in enumerate(zip(fracs, sizes))], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRI23IILEMm2"
      },
      "source": [
        "Which shows that a test dataset size of $2^{14}$ covers only about 0.045% of the dataset while a test datasize of $2^{23}$ covers roughly 23% of the entire dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuAZFuIYIxJy"
      },
      "source": [
        "## Standard Error of the Mean\n",
        "\n",
        "With this information in place, you are ready to find out the standard error of the mean across the range of the sample sizes for the individual columns in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTbHbqXsEhuy"
      },
      "source": [
        "import numpy as np\n",
        "def sem_over_range(lower, upper, mu, sigma):\n",
        "  sizes_series = pd.Series([2 ** i for i in range(lower, upper + 1)])\n",
        "  est_sem_df = pd.DataFrame( np.outer( (1 / np.sqrt(sizes_series)), sigma.values ), \n",
        "                        columns = sigma.index, \n",
        "                        index = sizes_series.values)\n",
        "  return est_sem_df\n",
        "\n",
        "sem_df = sem_over_range(sample_size_lower, sample_size_upper, mu, sigma)  \n",
        "sem_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAevp_VAFG3T"
      },
      "source": [
        "On a standard, linear plot, the standard error of the mean declines exponentially as the sample size doubles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwN-ji2PFHBW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize = (12, 9))\n",
        "plt.plot(sem_df.index, sem_df.mean(axis = 1))\n",
        "plt.xticks(sem_df.index, \n",
        "           labels = list(map(lambda i: f\"2^{i}\", np.log2(sem_df.index.values).astype(int))), \n",
        "           rotation = 90);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46kyuRWWFM4B"
      },
      "source": [
        "To discover the point of diminishing returns (the marginal) on the doubling of the sample size, you can start by looking at the total reduction in the standard error of the mean for each increase in the sample size. This is computed by\n",
        "\n",
        "`sem_df.cumsum()`\n",
        "\n",
        "in the following code snippet.\n",
        "\n",
        "Next, to obtain a single, aggregate measure for each sample size, the `mean(axis = 1)` computes the average of the total reduction in the standard error of the mean across the columns in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2KVzS-MFM-t"
      },
      "source": [
        "agg_change = sem_df.cumsum().mean(axis = 1)\n",
        "agg_change"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg9iAYUWFU6w"
      },
      "source": [
        "The point of diminishing returns (the marginal) can also be described as the point on the curve that is the furthest from the imaginary line connecting the first and the last point on the curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSyV9Wf9FVBj"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize = (12, 9))\n",
        "plt.scatter(agg_change.index, agg_change)\n",
        "plt.plot(agg_change.index, agg_change)\n",
        "plt.xticks(sem_df.index.values, \n",
        "           labels = list(map(lambda i: f\"2^{i}\", np.log2(sem_df.index.values).astype(int))), \n",
        "           rotation = 90);\n",
        "# plt.xticks(agg_change.index, labels = []);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-tqhq_6FWrA"
      },
      "source": [
        "The following computes the marginal (using the `marginal` function) and assigns it to the sample size value:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9E4VUdXFW9d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def marginal(x):\n",
        "  coor = np.vstack([x.index.values, x.values]).transpose()\n",
        "  return pd.Series(index = x.index, data = np.cross(coor[-1] - coor[0], coor[-1] - coor) / np.linalg.norm(coor[-1] - coor[0])).idxmin()\n",
        "\n",
        "SAMPLE_SIZE = marginal(agg_change).astype(int)\n",
        "SAMPLE_SIZE, SAMPLE_SIZE / ds_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZhiJDVdFd5A"
      },
      "source": [
        "Since the computed `SAMPLE_SIZE` is need to launch PySpark job, it is saved to an environment variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZuPk9V5FeAJ"
      },
      "source": [
        "os.environ['SAMPLE_SIZE'] = str(SAMPLE_SIZE)\n",
        "os.environ['SAMPLE_SIZE']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ewh0n6fFpGO"
      },
      "source": [
        "## Download a Utility Script to Run PySpark Jobs\n",
        "\n",
        "The script is downloaded as `utils.sh` and is loaded in the upcoming cells using `source utils.sh` command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yG6ivxgFqG-"
      },
      "source": [
        "%%bash\n",
        "wget -q --no-cache https://raw.githubusercontent.com/osipov/smlbook/master/utils.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swyoQ9UOFroK"
      },
      "source": [
        "## Use a PySpark job to sample the test set\n",
        "* the job also saves statistical summaries, including the z-scores and p-values of the sample test sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZFUcAbSV45w"
      },
      "source": [
        "In case by an unlucky draw, the p-value of the test set may be less than 0.05. The `SAMPLE_COUNT` parameter enables the PySpark job to sample a fair sample for up to `SAMPLE_COUNT` times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsztQSbzFmFv"
      },
      "source": [
        "os.environ['SAMPLE_COUNT'] = str(1)\n",
        "os.environ['SAMPLE_COUNT']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBhY0AULNR--"
      },
      "source": [
        "The next cell uses the Jupyter `%%writefile` magic to save the source code for the PySpark job to the `dctaxi_dev_test.py` file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf90peq_FrvA"
      },
      "source": [
        "%%writefile dctaxi_dev_test.py\n",
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "\n",
        "args = getResolvedOptions(sys.argv, ['JOB_NAME',\n",
        "                                     'BUCKET_SRC_PATH',\n",
        "                                     'BUCKET_DST_PATH',\n",
        "                                     'SAMPLE_SIZE',\n",
        "                                     'SAMPLE_COUNT',\n",
        "                                     'SEED'\n",
        "                                     ])\n",
        "\n",
        "sc = SparkContext()\n",
        "glueContext = GlueContext(sc)\n",
        "logger = glueContext.get_logger()\n",
        "spark = glueContext.spark_session\n",
        "\n",
        "job = Job(glueContext)\n",
        "job.init(args['JOB_NAME'], args)\n",
        "\n",
        "BUCKET_SRC_PATH = args['BUCKET_SRC_PATH']\n",
        "df = ( spark.read.format(\"parquet\")\n",
        "        .option(\"inferSchema\", \"true\")\n",
        "        .load(\"{}\".format(BUCKET_SRC_PATH))\n",
        "        .cache() )\n",
        "\n",
        "SAMPLE_SIZE = float( args['SAMPLE_SIZE'] )\n",
        "dataset_size = float( df.count() )\n",
        "sample_frac = SAMPLE_SIZE / dataset_size\n",
        "\n",
        "from kaen.spark import spark_df_to_stats_pandas_df, \\\n",
        "                      pandas_df_to_spark_df, \\\n",
        "                      spark_df_to_shards_df\n",
        "\n",
        "summary_df = spark_df_to_stats_pandas_df(df)\n",
        "mu = summary_df.loc['mean']\n",
        "sigma = summary_df.loc['stddev']\n",
        "\n",
        "SEED = int(args['SEED'])\n",
        "SAMPLE_COUNT = int(args['SAMPLE_COUNT'])\n",
        "BUCKET_DST_PATH = args['BUCKET_DST_PATH']\n",
        "\n",
        "for idx in range(SAMPLE_COUNT):\n",
        "  dev_df, test_df = ( df\n",
        "                      .cache()\n",
        "                      .randomSplit( [1.0 - sample_frac, sample_frac],\n",
        "                                    seed = SEED) )\n",
        "  test_df = test_df.limit( int(SAMPLE_SIZE) )\n",
        "\n",
        "  test_stats_df = \\\n",
        "    spark_df_to_stats_pandas_df(test_df, summary_df, pvalues = True, zscores = True)\n",
        "  \n",
        "  pvalues_series = test_stats_df.loc['pvalues']\n",
        "  if pvalues_series.min() > 0.05:\n",
        "    for df, desc in [(dev_df, \"dev\"), (test_df, \"test\")]:\n",
        "      ( df\n",
        "        .write\n",
        "        .option('header', 'true')\n",
        "        .csv(f\"{BUCKET_DST_PATH}/{desc}\", mode=\"overwrite\") )\n",
        "\n",
        "      stats_pandas_df = \\\n",
        "        spark_df_to_stats_pandas_df(df, summary_df, pvalues = True, zscores = True)\n",
        "\n",
        "      ( pandas_df_to_spark_df(spark,  stats_pandas_df)\n",
        "        .coalesce(1)\n",
        "        .write\n",
        "        .option('header', 'true')\n",
        "        .csv(f\"{BUCKET_DST_PATH}/{desc}/.meta/stats\", mode=\"overwrite\") )\n",
        "      \n",
        "      ( spark_df_to_shards_df(spark, df)\n",
        "        .coalesce(1)\n",
        "        .write\n",
        "        .option('header', True)\n",
        "        .csv(f\"{BUCKET_DST_PATH}/{desc}/.meta/shards\", mode='overwrite') )\n",
        "        \n",
        "    break\n",
        "  else:\n",
        "    SEED = SEED + idx\n",
        "      \n",
        "job.commit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGUgoyPGFzvW"
      },
      "source": [
        "## Run the PySpark job as specified by `dctaxi_dev_test.py`.\n",
        "* **the job should take about 8 minutes to finish**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByUpEt6LFz2v"
      },
      "source": [
        "%%bash\n",
        "source utils.sh\n",
        "\n",
        "PYSPARK_SRC_NAME=dctaxi_dev_test.py \\\n",
        "PYSPARK_JOB_NAME=dc-taxi-dev-test-job \\\n",
        "ADDITIONAL_PYTHON_MODULES=\"kaen[spark]\" \\\n",
        "BUCKET_SRC_PATH=s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/parquet/clean \\\n",
        "BUCKET_DST_PATH=s3://dc-taxi-$BUCKET_ID-$AWS_DEFAULT_REGION/csv \\\n",
        "SAMPLE_SIZE=$SAMPLE_SIZE \\\n",
        "SAMPLE_COUNT=1 \\\n",
        "SEED=68 \\\n",
        "run_job"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEUCVt5YMyFP"
      },
      "source": [
        "Once the job finishes successfully, you can review the metadata with summary statistics about the test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8Xyg4hfMnJF"
      },
      "source": [
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "\n",
        "test_stats_df = pd.read_csv(f\"s3://dc-taxi-{os.environ['BUCKET_ID']}-{os.environ['AWS_DEFAULT_REGION']}/csv/test/.meta/stats/*\")\n",
        "test_stats_df = test_stats_df.set_index('summary')\n",
        "test_stats_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Povs_p6WM4_d"
      },
      "source": [
        "or the metadata about how the PySpark job sharded the development dataset into objects with specific IDs and different number of records per object:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jntjONbMMnnm"
      },
      "source": [
        "dev_shards_df = pd.read_csv(f\"s3://dc-taxi-{os.environ['BUCKET_ID']}-{os.environ['AWS_DEFAULT_REGION']}/csv/dev/.meta/shards/*\")\n",
        "dev_shards_df = dev_shards_df.set_index('id')\n",
        "dev_shards_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYt-3VQUk5jv"
      },
      "source": [
        "Copyright 2021 CounterFactual.AI LLC. All Rights Reserved.\n",
        "\n",
        "Licensed under the GNU General Public License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. \n",
        "\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://github.com/osipov/smlbook/blob/master/LICENSE\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
      ]
    }
  ]
}